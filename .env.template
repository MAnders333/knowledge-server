# Knowledge Server Configuration
# Copy this to .env and fill in your values.

# Required: API key for the unified endpoint (used for both LLM and embedding calls)
LLM_API_KEY=your-unified-endpoint-api-key

# Required: base URL for the unified LLM endpoint.
# Provider-specific paths are appended automatically:
#   anthropic/ → /anthropic/v1, google/ → /gemini/v1beta, openai/ → /openai/v1
LLM_BASE_ENDPOINT=https://your-llm-endpoint.example.com

# Consolidation model — prefix determines provider SDK and API format:
#   anthropic/... → Anthropic API via /anthropic/v1
#   google/...    → Gemini API via /gemini/v1beta
#   openai/...    → OpenAI API via /openai/v1
# LLM_MODEL=anthropic/claude-sonnet-4-6

# Embedding model (always uses OpenAI-compatible API via /openai/v1)
# EMBEDDING_MODEL=text-embedding-3-large
# EMBEDDING_DIMENSIONS=3072

# Server
# KNOWLEDGE_PORT=3179
# KNOWLEDGE_HOST=127.0.0.1

# Database paths
# KNOWLEDGE_DB_PATH=~/.local/share/knowledge-server/knowledge.db
# OPENCODE_DB_PATH=~/.local/share/opencode/opencode.db

# Decay tuning
# DECAY_ARCHIVE_THRESHOLD=0.15
# DECAY_TOMBSTONE_DAYS=180

# Consolidation tuning
# CONSOLIDATION_CHUNK_SIZE=10
# CONSOLIDATION_MAX_SESSIONS=50
# CONSOLIDATION_MIN_MESSAGES=4

# Activation tuning
# ACTIVATION_MAX_RESULTS=10
# ACTIVATION_SIMILARITY_THRESHOLD=0.3
